{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'XML2MySQL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ed75ca521622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmysql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMySQLConnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mXML2MySQL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mMySQL2PandasDF\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMySQL2PandasDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'XML2MySQL'"
     ]
    }
   ],
   "source": [
    "#Dev\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from mysql.connector import MySQLConnection, Error\n",
    "from XML2MySQL import connect\n",
    "from MySQL2PandasDF import MySQL2PandasDF\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Session Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = MySQL2PandasDF(host = 'localhost', db = 'stackexchange_travel', user = 'root', password = password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db2df method parses the entirety of a database to dict of pandas dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.db2df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underlying structure is a dictionary with the following format:\n",
    "\n",
    "{Table Name: df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables in our dictionary\n",
    "\n",
    "session.source.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head of every table in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['badges'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['comments'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['posthistory'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['postlinks'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['posts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['tags'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['users'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['votes'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse columns with html present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session.parse_html('posts','Body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract tags from < >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def extract_tags_from_aligators(input_str_pandas_Series):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function extracts strings from the depths of two alligators' teeth <tag> in a pandas Series\n",
    "    New pandas Series 'Text_Search_Space' will be returned\n",
    "    \n",
    "    :params:     input_str_pandas_Series:              pandas.core.series.Series\n",
    "                \n",
    "    :return:     text_search_space_pandas_Series:      pandas.core.series.Series\n",
    "    \n",
    "    \"\"\"\n",
    "    return input_str_pandas_Series.str.findall(r\"<(:?.+)>\").str.join('').str.replace('>','').str.replace('<',' ').str.replace(r\"[^\\w\\s]\", ' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['posts']['Tags_tags_removed'] = extract_tags_from_aligators(session.source['posts']['Tags'])\n",
    "session.source['posts']['Tags_tags_removed'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new search space feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_str_fill_in_nans_pandas_Series(input_str_pandas_Series, nan_filler = ''):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function fills nans in a pandas Series with an optional filler\n",
    "    Input pandas Series will be returned with nans filled\n",
    "    \n",
    "    :params:     input_str_pandas_Series:                pandas.core.series.Series\n",
    "                 nan_filler:                             str\n",
    "                \n",
    "    :return:     input_str_pandas_Series_nans_filled:    pandas.core.series.Series\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert str(type(input_str_pandas_Series)) == \"<class 'pandas.core.series.Series'>\"\n",
    "    assert str(type(nan_filler)) == \"<class 'str'>\"\n",
    "    \n",
    "    return input_str_pandas_Series.fillna(nan_filler).map(str)\n",
    "\n",
    "def create_search_space(seperator, lower_case, input_str_pandas_Series1, input_str_pandas_Series2, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function concats pandas Series together with an optional seperator\n",
    "    A minimum of two pandas Series must be provided\n",
    "    Additional pandas_Series up to 5 can be used as inputs\n",
    "    New pandas Series 'Text_Search_Space' will be returned\n",
    "    \n",
    "    \n",
    "    :params:     seperator:                              str\n",
    "                 lower_case:                             Boolean\n",
    "                 input_str_pandas_Series1:               pandas.core.series.Series\n",
    "                 input_str_pandas_Series2:               pandas.core.series.Series\n",
    "                 input_str_pandas_Series_n:              pandas.core.series.Series\n",
    "                \n",
    "    :return:     text_search_space_pandas_Series:        pandas.core.series.Series\n",
    "    \n",
    "    \"\"\"\n",
    "    text_search_space_pandas_Series = pd.Series()\n",
    "    \n",
    "    assert str(type(input_str_pandas_Series1)) == \"<class 'pandas.core.series.Series'>\" and str(type(input_str_pandas_Series2)) == \"<class 'pandas.core.series.Series'>\"\n",
    "    assert str(type(seperator)) == \"<class 'str'>\"\n",
    "    assert str(type(lower_case)) == \"<class 'bool'>\"\n",
    "    \n",
    "    if lower_case is True: \n",
    "        text_search_space_pandas_Series = pd.Series(make_str_fill_in_nans_pandas_Series(input_str_pandas_Series1).str.lower().values \\\n",
    "                                                + seperator + make_str_fill_in_nans_pandas_Series(input_str_pandas_Series2).str.lower().values)\n",
    "    else:\n",
    "        text_search_space_pandas_Series = pd.Series(make_str_fill_in_nans_pandas_Series(input_str_pandas_Series1).values \\\n",
    "                                                + seperator + make_str_fill_in_nans_pandas_Series(input_str_pandas_Series2).values)\n",
    "    \n",
    "    for input_pandas_series_n in [i for i in args if str(type(i)) == \"<class 'pandas.core.series.Series'>\"]:\n",
    "        if lower_case is True:\n",
    "            text_search_space_pandas_Series = pd.Series(text_search_space_pandas_Series.values + seperator + make_str_fill_in_nans_pandas_Series(input_pandas_series_n).str.lower().values)\n",
    "        else:\n",
    "            text_search_space_pandas_Series = pd.Series(text_search_space_pandas_Series.values + seperator + make_str_fill_in_nans_pandas_Series(input_pandas_series_n).values)\n",
    "    \n",
    "    return text_search_space_pandas_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['posts']['Search_Space'] = create_search_space(' ', True, session.source['posts']['Tags_tags_removed'], session.source['posts']['Title'], session.source['posts']['Body'])\n",
    "session.source['posts']['Search_Space'].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    \n",
    "    :params:   text:    str\n",
    "    :return:   text:    str\n",
    "    \n",
    "    '''\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.source['posts']['Search_Space_stopwords_removed'] = session.source['posts']['Search_Space'].apply(lambda x: remove_stop_words(x))\n",
    "session.source['posts']['Search_Space_stopwords_removed'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make lemmanade out of the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Reduce all words down to lemma (root word)\n",
    "    2. Return lemmatized words\n",
    "    \n",
    "    :params:   text:   str\n",
    "    :return:   text:   str\n",
    "    \n",
    "    '''\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemma = WordNetLemmatizer()\n",
    "    normalized = \" \".join(lemma.lemmatize(word,'v') for word in text.split())\n",
    "    x = normalized.split()\n",
    "    y = [s for s in x if len(s) > 2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.source['posts']['Search_Space_stopwords_removed_lemma'] = session.source['posts']['Search_Space_stopwords_removed'].apply(lambda x: lemma(x))\n",
    "\" \".join(session.source['posts']['Search_Space_stopwords_removed_lemma'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dictionary to Feed to LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffe\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.6-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6d96e3e333c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlist_of_post_contents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpost_contents\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Search_Space_stopwords_removed_lemma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlist_of_post_contents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_contents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.6-win-amd64.egg\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.6-win-amd64.egg\\gensim\\matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_stats_mstats_common\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_find_repeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinregress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheilslopes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jeffe\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "list_of_post_contents = list()\n",
    "for post_contents in session.source['posts']['Search_Space_stopwords_removed_lemma']:\n",
    "    list_of_post_contents.append(post_contents)\n",
    "\n",
    "dictionary = corpora.Dictionary(list_of_post_contents)\n",
    "\n",
    "# Filter terms which occurs in less than 4 articles & more than 50% of the articles \n",
    "dictionary.filter_extremes(no_below=4, no_above=0.5)\n",
    "\n",
    "# List of few words which are removed from dictionary as they are content neutral\n",
    "stoplist = set('also use make people know many call include part find become like mean often different \\\n",
    "               usually take wikt come give well get since type list say change see refer actually iii \\\n",
    "               aisne kinds pas ask would way something need things want every str'.split())\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in list_of_post_contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel as Lda\n",
    "# Creating the object for LDA model using gensim library & Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=20, id2word = dictionary, passes=50, iterations=500)\n",
    "\n",
    "# Print all the 20 topics\n",
    "for i,topic in enumerate(ldamodel.print_topics(num_topics=20, num_words=10)):\n",
    "    words = topic[1].split(\"+\")\n",
    "    print(words,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing intertopic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(p, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
